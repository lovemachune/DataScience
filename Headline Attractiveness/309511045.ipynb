{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, Conv2D, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_split(mystring):\n",
    "    str_split = []\n",
    "    #nltk_stopwords= nltk.corpus.stopwords.words('english')\n",
    "    for tmp in mystring:\n",
    "        tmp = tmp.lower()\n",
    "        #punct_token = wordpunct_tokenize(tmp)\n",
    "        tmp = re.sub('[^a-zA-Z0-9\\s\\?\\!]+', '', tmp)\n",
    "        tmp = tmp.replace('!', ' !')\n",
    "        tmp = tmp.replace('?', ' ?')\n",
    "        tmp = tmp.split(' ')\n",
    "        while True:\n",
    "            if '' not in tmp:\n",
    "                break\n",
    "            tmp.remove('')\n",
    "        while True:\n",
    "            if 'the' not in tmp:\n",
    "                break\n",
    "            tmp.remove('the')\n",
    "        while True:\n",
    "            if 'and' not in tmp:\n",
    "                break\n",
    "            tmp.remove('and')\n",
    "        while True:\n",
    "            if 'of' not in tmp:\n",
    "                break\n",
    "            tmp.remove('of')\n",
    "        '''\n",
    "        while True:\n",
    "            if 'is' not in tmp:\n",
    "                break\n",
    "            tmp.remove('is')\n",
    "        while True:\n",
    "            if 'are' not in tmp:\n",
    "                break\n",
    "            tmp.remove('are')\n",
    "        '''\n",
    "        str_split.append(tmp)\n",
    "    return str_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = './'\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "NUM_LSTM_UNITS = 512\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(\"train.csv\")\n",
    "data = all_data['Headline']\n",
    "label = all_data['Label']\n",
    "my_split = word_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(my_split)\n",
    "sequences = tokenizer.texts_to_sequences(my_split)\n",
    "word_index = tokenizer.word_index\n",
    "x = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = keras.models.load_model('./mymodel_0.445/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3372777]\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "data = test_data['Headline']\n",
    "label = test_data['Label']\n",
    "test_split = word_split(data)\n",
    "sequences = tokenizer.texts_to_sequences(test_split)\n",
    "x = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_pre = mm.predict(x)\n",
    "print(y_pre[0])\n",
    "b = np.arange(1, y_pre.shape[0]+1).reshape(y_pre.shape[0], 1).astype('int32')\n",
    "y_pre = np.append(b, y_pre, axis=1).astype(object)\n",
    "for i in range(len(y_pre)):\n",
    "    y_pre[i][0] = int(y_pre[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['ID','Label'])\n",
    "    writer.writerows(y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3372777]\n"
     ]
    }
   ],
   "source": [
    "mn = keras.models.load_model('my_model.h5')\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "data = test_data['Headline']\n",
    "label = test_data['Label']\n",
    "test_split = word_split(data)\n",
    "sequences = tokenizer.texts_to_sequences(test_split)\n",
    "x = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_pre = mm.predict(x)\n",
    "print(y_pre[0])\n",
    "b = np.arange(1, y_pre.shape[0]+1).reshape(y_pre.shape[0], 1).astype('int32')\n",
    "y_pre = np.append(b, y_pre, axis=1).astype(object)\n",
    "for i in range(len(y_pre)):\n",
    "    y_pre[i][0] = int(y_pre[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID     Label\n",
      "0      1  2.337278\n",
      "1      2  2.964703\n",
      "2      3  2.684707\n",
      "3      4  2.922065\n",
      "4      5  2.455397\n",
      "..   ...       ...\n",
      "222  223  3.060324\n",
      "223  224  2.522440\n",
      "224  225  2.865470\n",
      "225  226  2.824675\n",
      "226  227  2.668736\n",
      "\n",
      "[227 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2.4667890071868896]\n",
      " [2 2.4667890071868896]\n",
      " [3 2.4667890071868896]\n",
      " [4 2.4667890071868896]\n",
      " [5 2.4667890071868896]\n",
      " [6 2.4667890071868896]\n",
      " [7 2.4667890071868896]\n",
      " [8 2.4667890071868896]\n",
      " [9 2.4667890071868896]\n",
      " [10 2.4667890071868896]\n",
      " [11 2.4667890071868896]\n",
      " [12 2.4667890071868896]\n",
      " [13 2.4667890071868896]\n",
      " [14 2.4667890071868896]\n",
      " [15 2.4667890071868896]\n",
      " [16 2.4667890071868896]\n",
      " [17 2.4667890071868896]\n",
      " [18 2.4667890071868896]\n",
      " [19 2.4667890071868896]\n",
      " [20 2.4667890071868896]\n",
      " [21 2.4667890071868896]\n",
      " [22 2.4667890071868896]\n",
      " [23 2.4667890071868896]\n",
      " [24 2.4667890071868896]\n",
      " [25 2.4667890071868896]\n",
      " [26 2.4667890071868896]\n",
      " [27 2.4667890071868896]\n",
      " [28 2.4667890071868896]\n",
      " [29 2.4667890071868896]\n",
      " [30 2.4667890071868896]\n",
      " [31 2.4667890071868896]\n",
      " [32 2.4667890071868896]\n",
      " [33 2.4667890071868896]\n",
      " [34 2.4667890071868896]\n",
      " [35 2.4667890071868896]\n",
      " [36 2.4667890071868896]\n",
      " [37 2.4667890071868896]\n",
      " [38 2.4667890071868896]\n",
      " [39 2.4667890071868896]\n",
      " [40 2.4667890071868896]\n",
      " [41 2.4667890071868896]\n",
      " [42 2.4667890071868896]\n",
      " [43 2.4667890071868896]\n",
      " [44 2.4667890071868896]\n",
      " [45 2.4667890071868896]\n",
      " [46 2.4667890071868896]\n",
      " [47 2.4667890071868896]\n",
      " [48 2.4667890071868896]\n",
      " [49 2.4667890071868896]\n",
      " [50 2.4667890071868896]\n",
      " [51 2.4667890071868896]\n",
      " [52 2.4667890071868896]\n",
      " [53 2.4667890071868896]\n",
      " [54 2.4667890071868896]\n",
      " [55 2.4667890071868896]\n",
      " [56 2.4667890071868896]\n",
      " [57 2.4667890071868896]\n",
      " [58 2.4667890071868896]\n",
      " [59 2.4667890071868896]\n",
      " [60 2.4667890071868896]\n",
      " [61 2.4667890071868896]\n",
      " [62 2.4667890071868896]\n",
      " [63 2.4667890071868896]\n",
      " [64 2.4667890071868896]\n",
      " [65 2.4667890071868896]\n",
      " [66 2.4667890071868896]\n",
      " [67 2.4667890071868896]\n",
      " [68 2.4667890071868896]\n",
      " [69 2.4667890071868896]\n",
      " [70 2.4667890071868896]\n",
      " [71 2.4667890071868896]\n",
      " [72 2.4667890071868896]\n",
      " [73 2.4667890071868896]\n",
      " [74 2.4667890071868896]\n",
      " [75 2.4667890071868896]\n",
      " [76 2.4667890071868896]\n",
      " [77 2.4667890071868896]\n",
      " [78 2.4667890071868896]\n",
      " [79 2.4667890071868896]\n",
      " [80 2.4667890071868896]\n",
      " [81 2.4667890071868896]\n",
      " [82 2.4667890071868896]\n",
      " [83 2.4667890071868896]\n",
      " [84 2.4667890071868896]\n",
      " [85 2.4667890071868896]\n",
      " [86 2.4667890071868896]\n",
      " [87 2.4667890071868896]\n",
      " [88 2.4667890071868896]\n",
      " [89 2.4667890071868896]\n",
      " [90 2.4667890071868896]\n",
      " [91 2.4667890071868896]\n",
      " [92 2.4667890071868896]\n",
      " [93 2.4667890071868896]\n",
      " [94 2.4667890071868896]\n",
      " [95 2.4667890071868896]\n",
      " [96 2.4667890071868896]\n",
      " [97 2.4667890071868896]\n",
      " [98 2.4667890071868896]\n",
      " [99 2.4667890071868896]\n",
      " [100 2.4667890071868896]\n",
      " [101 2.4667890071868896]\n",
      " [102 2.4667890071868896]\n",
      " [103 2.4667890071868896]\n",
      " [104 2.4667890071868896]\n",
      " [105 2.4667890071868896]\n",
      " [106 2.4667890071868896]\n",
      " [107 2.4667890071868896]\n",
      " [108 2.4667890071868896]\n",
      " [109 2.4667890071868896]\n",
      " [110 2.4667890071868896]\n",
      " [111 2.4667890071868896]\n",
      " [112 2.4667890071868896]\n",
      " [113 2.4667890071868896]\n",
      " [114 2.4667890071868896]\n",
      " [115 2.4667890071868896]\n",
      " [116 2.4667890071868896]\n",
      " [117 2.4667890071868896]\n",
      " [118 2.4667890071868896]\n",
      " [119 2.4667890071868896]\n",
      " [120 2.4667890071868896]\n",
      " [121 2.4667890071868896]\n",
      " [122 2.4667890071868896]\n",
      " [123 2.4667890071868896]\n",
      " [124 2.4667890071868896]\n",
      " [125 2.4667890071868896]\n",
      " [126 2.4667890071868896]\n",
      " [127 2.4667890071868896]\n",
      " [128 2.4667890071868896]\n",
      " [129 2.4667890071868896]\n",
      " [130 2.4667890071868896]\n",
      " [131 2.4667890071868896]\n",
      " [132 2.4667890071868896]\n",
      " [133 2.4667890071868896]\n",
      " [134 2.4667890071868896]\n",
      " [135 2.4667890071868896]\n",
      " [136 2.4667890071868896]\n",
      " [137 2.4667890071868896]\n",
      " [138 2.4667890071868896]\n",
      " [139 2.4667890071868896]\n",
      " [140 2.4667890071868896]\n",
      " [141 2.4667890071868896]\n",
      " [142 2.4667890071868896]\n",
      " [143 2.4667890071868896]\n",
      " [144 2.4667890071868896]\n",
      " [145 2.4667890071868896]\n",
      " [146 2.4667890071868896]\n",
      " [147 2.4667890071868896]\n",
      " [148 2.4667890071868896]\n",
      " [149 2.4667890071868896]\n",
      " [150 2.4667890071868896]\n",
      " [151 2.4667890071868896]\n",
      " [152 2.4667890071868896]\n",
      " [153 2.4667890071868896]\n",
      " [154 2.4667890071868896]\n",
      " [155 2.4667890071868896]\n",
      " [156 2.4667890071868896]\n",
      " [157 2.4667890071868896]\n",
      " [158 2.4667890071868896]\n",
      " [159 2.4667890071868896]\n",
      " [160 2.4667890071868896]\n",
      " [161 2.4667890071868896]\n",
      " [162 2.4667890071868896]\n",
      " [163 2.4667890071868896]\n",
      " [164 2.4667890071868896]\n",
      " [165 2.4667890071868896]\n",
      " [166 2.4667890071868896]\n",
      " [167 2.4667890071868896]\n",
      " [168 2.4667890071868896]\n",
      " [169 2.4667890071868896]\n",
      " [170 2.4667890071868896]\n",
      " [171 2.4667890071868896]\n",
      " [172 2.4667890071868896]\n",
      " [173 2.4667890071868896]\n",
      " [174 2.4667890071868896]\n",
      " [175 2.4667890071868896]\n",
      " [176 2.4667890071868896]\n",
      " [177 2.4667890071868896]\n",
      " [178 2.4667890071868896]\n",
      " [179 2.4667890071868896]\n",
      " [180 2.4667890071868896]\n",
      " [181 2.4667890071868896]\n",
      " [182 2.4667890071868896]\n",
      " [183 2.4667890071868896]\n",
      " [184 2.4667890071868896]\n",
      " [185 2.4667890071868896]\n",
      " [186 2.4667890071868896]\n",
      " [187 2.4667890071868896]\n",
      " [188 2.4667890071868896]\n",
      " [189 2.4667890071868896]\n",
      " [190 2.4667890071868896]\n",
      " [191 2.4667890071868896]\n",
      " [192 2.4667890071868896]\n",
      " [193 2.4667890071868896]\n",
      " [194 2.4667890071868896]\n",
      " [195 2.4667890071868896]\n",
      " [196 2.4667890071868896]\n",
      " [197 2.4667890071868896]\n",
      " [198 2.4667890071868896]\n",
      " [199 2.4667890071868896]\n",
      " [200 2.4667890071868896]\n",
      " [201 2.4667890071868896]\n",
      " [202 2.4667890071868896]\n",
      " [203 2.4667890071868896]\n",
      " [204 2.4667890071868896]\n",
      " [205 2.4667890071868896]\n",
      " [206 2.4667890071868896]\n",
      " [207 2.4667890071868896]\n",
      " [208 2.4667890071868896]\n",
      " [209 2.4667890071868896]\n",
      " [210 2.4667890071868896]\n",
      " [211 2.4667890071868896]\n",
      " [212 2.4667890071868896]\n",
      " [213 2.4667890071868896]\n",
      " [214 2.4667890071868896]\n",
      " [215 2.4667890071868896]\n",
      " [216 2.4667890071868896]\n",
      " [217 2.4667890071868896]\n",
      " [218 2.4667890071868896]\n",
      " [219 2.4667890071868896]\n",
      " [220 2.4667890071868896]\n",
      " [221 2.4667890071868896]\n",
      " [222 2.4667890071868896]\n",
      " [223 2.4667890071868896]\n",
      " [224 2.4667890071868896]\n",
      " [225 2.4667890071868896]\n",
      " [226 2.4667890071868896]\n",
      " [227 2.4667890071868896]]\n"
     ]
    }
   ],
   "source": [
    "print((y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
